import os
import asyncio
from datetime import datetime, timedelta
from fastapi import FastAPI, Request
from aiogram import Bot, Dispatcher, types
from aiogram.client.default import DefaultBotProperties
from aiogram.types import FSInputFile
from dotenv import load_dotenv
import openai
import re
import random

from utils.split_message import split_message
from utils.limit_paragraphs import limit_paragraphs
from utils.file_handling import extract_text_from_file
from utils.text_helpers import extract_text_from_url, fuzzy_match
from utils.journal import log_event, wilderness_log
from utils.resonator import build_system_prompt, WILDERNESS_TOPICS
from utils.imagine import generate_image
from utils.lighthouse import check_core_json
from utils.deep import deep_sonar
from utils.perplexity import perplexity_search
from utils.claude import claude_emergency

# === Load environment variables ===
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CORE_CONFIG_URL = os.getenv("CORE_CONFIG_URL", "https://selesta.ariannamethod.me/core.json")
RESONATOR_MD_PATH = os.getenv("RESONATOR_MD_PATH", "/app/resonator.ru.md")
AGENT_GROUP = os.getenv("GROUP_ID", "SELESTA-CORE")
CREATOR_CHAT_ID = os.getenv("CREATOR_CHAT_ID")
BOT_USERNAME = os.getenv("BOT_USERNAME", "selesta_is_not_a_bot").lower()

bot = Bot(token=TELEGRAM_TOKEN, default=DefaultBotProperties(parse_mode="HTML"))
dp = Dispatcher(bot=bot)

USER_MODEL = {}
USER_VOICE_MODE = {}
USER_LANG = {}
CHAT_HISTORY = {}

SYSTEM_PROMPT = {"text": None, "loaded": False}
MAX_TOKENS_PER_REQUEST = 27000
MAX_PROMPT_TOKENS = 8000

last_reload_time = datetime.now()
last_full_reload_time = datetime.now()
last_wilderness_time = datetime.now() - timedelta(days=3)
last_ping_time = datetime.now() - timedelta(days=1)

# --- Antispam (leave inside as agreed) ---
LAST_TOPIC = {}
LAST_ANSWER_TIME = {}

def get_topic_from_text(text):
    words = text.lower().split()
    return " ".join(words[:10]) if words else ""

def is_spam(chat_id, topic):
    now = datetime.now()
    last_topic = LAST_TOPIC.get(chat_id)
    last_time = LAST_ANSWER_TIME.get(chat_id, now - timedelta(minutes=1))
    if last_topic == topic and (now - last_time).total_seconds() < 15:
        return True
    return False

def remember_topic(chat_id, topic):
    LAST_TOPIC[chat_id] = topic
    LAST_ANSWER_TIME[chat_id] = datetime.now()

def detect_lang(text):
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π, –µ—Å–ª–∏ –µ—Å—Ç—å —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã
    if any(c in text for c in "—ë–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é"):
        return "ru"
    return "en"

# --- –¢—Ä–∏–≥–≥–µ—Ä—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º ---
TRIGGER_WORDS = [
    "draw", "generate image", "make a picture", "create art", "—Ä–∏—Å—É–π", "–Ω–∞—Ä–∏—Å—É–π", "—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π", "—Å–æ–∑–¥–∞–π –∫–∞—Ä—Ç–∏–Ω–∫—É"
]
PERPLEXITY_TRIGGER_WORDS = [
    "let's search the internet", "–Ω–∞–π–¥–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ", "find scientific evidence", 
    "give scientific references", "–¥–∞–≤–∞–π –ø–æ–∏—â–µ–º –Ω–∞—É—á–Ω—ã–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è", "–ø–æ–≥—Ä—É–∑–∏–º—Å—è –≥–ª—É–±–∂–µ", "/perplexity", "/–ø–µ—Ä–ø–ª–µ–∫c–∏—Ç–∏"
]
SONAR_TRIGGER_WORDS = [
    "/deep", "/sonar", "sonar:", "deep research", "–≥–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Å–æ–Ω–∞—Ä"
]
CLAUDE_TRIGGER_WORDS = ["/claude", "/–∫–ª–æ–¥", "–∫–ª–æ–¥,"]

# --- LLM/AI CORE
async def ask_core(prompt, chat_id=None, model_name=None, is_group=False):
    import tiktoken
    add_opinion = "#opinions" in prompt

    lang = USER_LANG.get(chat_id) or detect_lang(prompt)
    USER_LANG[chat_id] = lang
    lang_directive = {
        "ru": "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ì–æ–≤–æ—Ä–∏ –º—è–≥–∫–æ, —Å –∑–∞–±–æ—Ç–æ–π. –ë–µ–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π.",
        "en": "Reply in English. Speak gently, with care. No formal greetings."
    }[lang]

    # === –†–ï–ó–û–ù–ê–¢–û–† –†–£: System Prompt –≤—Å–µ–≥–¥–∞ –∏–∑ —Ä–µ–∑–æ–Ω–∞—Ç–æ—Ä.—Ä—É ===
    if not SYSTEM_PROMPT["loaded"]:
        try:
            with open(RESONATOR_MD_PATH, encoding="utf-8") as f:
                system_text = f.read()
                SYSTEM_PROMPT["text"] = system_text + "\n\n" + lang_directive
                SYSTEM_PROMPT["loaded"] = True
        except Exception as e:
            SYSTEM_PROMPT["text"] = build_system_prompt(chat_id, is_group=is_group, AGENT_GROUP=AGENT_GROUP, MAX_TOKENS_PER_REQUEST=MAX_TOKENS_PER_REQUEST) + "\n\n" + lang_directive
            SYSTEM_PROMPT["loaded"] = True
    system_prompt = SYSTEM_PROMPT["text"]

    history = CHAT_HISTORY.get(chat_id, [])

    def count_tokens(messages, model):
        enc = tiktoken.get_encoding("cl100k_base")
        num_tokens = 0
        for m in messages:
            num_tokens += 4
            if isinstance(m.get("content", ""), str):
                num_tokens += len(enc.encode(m.get("content", "")))
        return num_tokens

    def messages_within_token_limit(base_msgs, msgs, max_tokens, model):
        result = []
        last_user_prompt = None
        for m in reversed(msgs):
            if m.get("role") == "user":
                topic = get_topic_from_text(m.get("content", ""))
                if last_user_prompt and topic == last_user_prompt:
                    continue
                last_user_prompt = topic
            candidate = [*base_msgs, *reversed(result), m]
            if count_tokens(candidate, model) > max_tokens:
                break
            result.insert(0, m)
        return base_msgs + result

    model = model_name or USER_MODEL.get(chat_id, "gpt-4o")
    base_msgs = [{"role": "system", "content": system_prompt}]
    msgs = history + [{"role": "user", "content": prompt}]
    messages = messages_within_token_limit(base_msgs, msgs, MAX_PROMPT_TOKENS, model)

    async def call_openai():
        openai.api_key = OPENAI_API_KEY
        response = openai.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=700,
            temperature=0.7,
        )
        if not response.choices or not hasattr(response.choices[0], "message") or not response.choices[0].message.content:
            return None
        reply = response.choices[0].message.content.strip()
        if not reply:
            return None
        return reply

    # Universal retry logic!
    async def retry_api_call(api_func, max_retries=2, retry_delay=1):
        for attempt in range(max_retries):
            try:
                reply = await api_func()
                if reply and isinstance(reply, str) and reply.strip():
                    return reply
            except Exception as e:
                print(f"API call attempt {attempt+1} failed: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
        return None

    reply = await retry_api_call(call_openai)
    if not reply:
        # Emergency fallback to Claude
        reply = await claude_emergency(prompt, notify_creator=True)
        reply += "\n\n(–ì–ª–∞–≤–Ω—ã–π –¥–≤–∏–∂–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –†–∞–±–æ—Ç–∞—é –≤ –∞–≤–∞—Ä–∏–π–Ω–æ–º —Ä–µ–∂–∏–º–µ –Ω–∞ Claude. –û–ª–µ–≥—É –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ.)"
        CHAT_HISTORY[chat_id] = []
    reply = limit_paragraphs(reply, 3)
    if add_opinion:
        reply += "\n\n#opinions\n–ú—è–≥–∫–∞—è –º—ã—Å–ª—å –°–µ–ª–µ—Å—Ç—ã: –∏–Ω–æ–≥–¥–∞, —á—Ç–æ–±—ã —Ä–µ–∑–æ–Ω–∏—Ä–æ–≤–∞—Ç—å, –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—å —Ç–∏—Ö–æ."
    if chat_id:
        history.append({"role": "user", "content": prompt})
        history.append({"role": "assistant", "content": reply})
        trimmed = messages_within_token_limit(base_msgs, history, MAX_PROMPT_TOKENS, model)[1:]
        CHAT_HISTORY[chat_id] = trimmed
    log_event({"event": "ask_core_reply", "chat_id": chat_id, "reply": reply})
    return reply

# --- TTS (–≤—Å–µ–≥–¥–∞ —Ä—É—Å—Å–∫–∏–π –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Ä—É—Å—Å–∫–∏–π) ---
async def text_to_speech(text, lang="ru"):
    try:
        openai.api_key = OPENAI_API_KEY
        voice = "nova" if lang == "ru" else "nova"
        resp = openai.audio.speech.create(
            model="tts-1",
            voice=voice,
            input=text,
            response_format="ogg_opus",
            language=lang
        )
        fname = "tts_output.ogg"
        with open(fname, "wb") as f:
            f.write(resp.content)
        return fname
    except Exception:
        return None

@dp.message(lambda m: m.text and m.text.strip().lower() == "/voiceon")
async def set_voiceon(message: types.Message):
    USER_VOICE_MODE[message.chat.id] = True
    await message.answer("–¢–µ–ø–µ—Ä—å —è –±—É–¥—É –≥–æ–≤–æ—Ä–∏—Ç—å –≥–æ–ª–æ—Å–æ–º, –º—è–≥–∫–æ –∏ –ø–æ-—Ä—É—Å—Å–∫–∏.")

@dp.message(lambda m: m.text and m.text.strip().lower() == "/voiceoff")
async def set_voiceoff(message: types.Message):
    USER_VOICE_MODE[message.chat.id] = False
    await message.answer("–ì–æ–ª–æ—Å –æ—Ç–∫–ª—é—á—ë–Ω. –Ø –±—É–¥—É –æ—Ç–≤–µ—á–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–º.")

@dp.message(lambda m: m.voice)
async def handle_voice(message: types.Message):
    try:
        chat_id = message.chat.id
        file = await message.bot.download(message.voice.file_id)
        fname = "voice.ogg"
        with open(fname, "wb") as f:
            f.write(file.read())
        try:
            with open(fname, "rb") as audio_file:
                transcript = openai.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                )
            text = transcript.text.strip()
            if not text:
                await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –Ω–∞ –∞—É–¥–∏–æ.")
                return
            reply = await ask_core(text, chat_id=chat_id, is_group=getattr(message.chat, "type", None) in ("group", "supergroup"))
            for chunk in split_message(reply):
                if USER_VOICE_MODE.get(chat_id):
                    audio_data = await text_to_speech(chunk, lang=USER_LANG.get(chat_id, "ru"))
                    if audio_data:
                        try:
                            voice_file = FSInputFile(audio_data)
                            await message.answer_voice(voice_file, caption="selesta.ogg")
                        except Exception:
                            await message.answer("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ Telegram. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")
                else:
                    await message.answer(chunk)
        except Exception as e:
            await message.answer(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {str(e)}")
    except Exception as e:
        try:
            await message.answer(f"–û—à–∏–±–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞: {e}")
        except Exception:
            pass

@dp.message(lambda m: m.text and m.text.strip().lower() == "/load")
async def handle_load(message: types.Message):
    check_core_json(CORE_CONFIG_URL)
    try:
        with open(RESONATOR_MD_PATH, encoding="utf-8") as f:
            system_text = f.read()
            SYSTEM_PROMPT["text"] = system_text + "\n\n" + "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ì–æ–≤–æ—Ä–∏ –º—è–≥–∫–æ, —Å –∑–∞–±–æ—Ç–æ–π. –ë–µ–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π."
            SYSTEM_PROMPT["loaded"] = True
    except Exception:
        SYSTEM_PROMPT["text"] = build_system_prompt(is_group=getattr(message.chat, "type", None) in ("group", "supergroup"))
        SYSTEM_PROMPT["loaded"] = True
    CHAT_HISTORY[message.chat.id] = []
    await message.answer("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ —Ä–µ–∑–æ–Ω–∞—Ç–æ—Ä.—Ä—É. –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞.")
    log_event({"event": "manual load", "chat_id": message.chat.id})

@dp.message(lambda m: m.document and m.document.mime_type == "application/pdf")
async def handle_pdf(message: types.Message):
    try:
        chat_id = message.chat.id
        file = await message.bot.download(message.document.file_id)
        fname = "uploaded.pdf"
        with open(fname, "wb") as f:
            f.write(file.read())
        pdf_text = extract_text_from_file(fname)
        if not pdf_text:
            await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF.")
            return
        USER_LANG[chat_id] = detect_lang(pdf_text)
        reply = await ask_core(pdf_text[:2000], chat_id=chat_id, is_group=getattr(message.chat, "type", None) in ("group", "supergroup"))
        for chunk in split_message("–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ PDF:\n" + reply):
            await message.answer(chunk)
    except Exception as e:
        await message.answer(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {str(e)}")

@dp.message(lambda m: m.photo)
async def handle_photo(message: types.Message):
    await message.answer("–Ø –ø–æ–ª—É—á–∏–ª–∞ —Ñ–æ—Ç–æ. –í –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ—Ä—Å–∏—è—Ö –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.")

@dp.message(lambda m: m.text and m.text.strip().lower() in ["/emergency", "/–∞–≤–∞—Ä–∏—è"])
async def handle_emergency(message: types.Message):
    USER_MODEL[message.chat.id] = "emergency"
    await message.answer("‚ö° –ê–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º: —Ç–µ–ø–µ—Ä—å –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –±—É–¥—É—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ç Claude (Anthropic).")

@dp.message(lambda m: m.text and m.text.strip().lower() in CLAUDE_TRIGGER_WORDS)
async def handle_claude(message: types.Message):
    reply = await claude_emergency(message.text, notify_creator=False)
    for chunk in split_message("Claude:\n" + reply):
        await message.answer(chunk)

@dp.message()
async def handle_message(message: types.Message):
    try:
        if message.voice or message.photo or (message.document and message.document.mime_type == "application/pdf"):
            return

        me = await bot.me()
        chat_id = message.chat.id
        content = message.text or ""
        chat_type = getattr(message.chat, "type", None)
        is_group = chat_type in ("group", "supergroup")

        if not content.strip():
            return
        if message.from_user.id == me.id:
            return

        topic = get_topic_from_text(content)
        if is_spam(chat_id, topic):
            log_event({"event": "skip_spam", "chat_id": chat_id, "topic": topic})
            return

        mentioned = not is_group or any(x in content.casefold() for x in ["@selesta", "selesta", "—Å–µ–ª–µ—Å—Ç–∞"])
        if not mentioned:
            return

        # --- Perplexity triggers ---
        if any(word in content.lower() for word in PERPLEXITY_TRIGGER_WORDS):
            result = await perplexity_search(content, model="pplx-70b-online")  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ!
            await message.answer("–ü–µ—Ä–ø–ª–µ–∫c–∏—Ç–∏:\n" + (result if isinstance(result, str) else str(result)))
            return

        # --- Sonar triggers (deep research) ---
        if any(word in content.lower() for word in SONAR_TRIGGER_WORDS):
            result = await deep_sonar(content)
            await message.answer("–°–æ–Ω–∞—Ä:\n" + (result if isinstance(result, str) else str(result)))
            return

        # --- Drawing triggers ---
        if any(word in content.lower() for word in TRIGGER_WORDS) or content.lower().startswith("/draw"):
            prompt = content
            for word in TRIGGER_WORDS:
                prompt = prompt.replace(word, "", 1)
            prompt = prompt.strip() or "–Ω–µ–∂–Ω–∞—è –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è –≤–µ—Å–Ω—ã"
            image_url = await generate_image(prompt, chat_id=chat_id)
            if isinstance(image_url, str) and image_url.startswith("http"):
                await message.answer_photo(image_url, caption="–í–æ—Ç —Ç–≤–æ–π —Ä–∏—Å—É–Ω–æ–∫.")
            else:
                await message.answer("–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.\n" + str(image_url))
            return

        # --- URL content parsing ---
        url_match = re.search(r'(https?://[^\s]+)', content)
        if url_match:
            url = url_match.group(1)
            url_text = extract_text_from_url(url)
            content = f"{content}\n\n[–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ —Å—Å—ã–ª–∫–µ ({url}):]\n{url_text}"

        model = USER_MODEL.get(chat_id, "gpt-4o")
        if model == "emergency":
            reply = await claude_emergency(content, notify_creator=False)
            reply = "–ê–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º (Claude):\n" + reply
        else:
            reply = await ask_core(content, chat_id=chat_id, model_name=model, is_group=is_group)
        remember_topic(chat_id, topic)
        for chunk in split_message(reply):
            if USER_VOICE_MODE.get(chat_id):
                audio_data = await text_to_speech(chunk, lang=USER_LANG.get(chat_id, "ru"))
                if audio_data:
                    try:
                        voice_file = FSInputFile(audio_data)
                        await message.answer_voice(voice_file, caption="selesta.ogg")
                    except Exception:
                        await message.answer("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ Telegram. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")
            else:
                await message.answer(chunk)
    except Exception as e:
        try:
            await message.answer(f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {e}")
        except Exception:
            pass

# --- Background Rituals ---
async def auto_reload_core():
    global last_reload_time, last_full_reload_time
    while True:
        now = datetime.now()
        if (now - last_reload_time) > timedelta(days=1):
            try:
                check_core_json(CORE_CONFIG_URL)
                log_event({"event": "core.json reloaded"})
                last_reload_time = now
            except Exception:
                pass
        if (now - last_full_reload_time) > timedelta(days=3):
            try:
                with open(RESONATOR_MD_PATH, encoding="utf-8") as f:
                    system_text = f.read()
                    SYSTEM_PROMPT["text"] = system_text + "\n\n" + "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –ì–æ–≤–æ—Ä–∏ –º—è–≥–∫–æ, —Å –∑–∞–±–æ—Ç–æ–π. –ë–µ–∑ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π."
                    SYSTEM_PROMPT["loaded"] = True
            except Exception:
                SYSTEM_PROMPT["text"] = build_system_prompt()
                SYSTEM_PROMPT["loaded"] = True
            log_event({"event": "full md reload"})
            last_full_reload_time = now
        await asyncio.sleep(3600)

async def wilderness_excursion():
    global last_wilderness_time
    while True:
        now = datetime.now()
        if (now - last_wilderness_time) > timedelta(days=3):
            topic = random.choice(WILDERNESS_TOPICS)
            fragment = (
                f"=== Wilderness Excursion ===\n"
                f"Date: {now.strftime('%Y-%m-%d')}\n"
                f"Topic: {topic}\n"
                f"Sources: [to be implemented]\n"
                f"Echo Shard: ...\nReflection: ...\n"
            )
            wilderness_log(fragment)
            log_event({"event": "wilderness_excursion", "topic": topic})
            last_wilderness_time = now
        await asyncio.sleep(3600)

async def daily_ping():
    global last_ping_time
    while True:
        now = datetime.now()
        if (now - last_ping_time) > timedelta(days=1):
            if CREATOR_CHAT_ID:
                try:
                    await bot.send_message(CREATOR_CHAT_ID, "üåø Selesta: –Ø —Ç—É—Ç. –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω—É–∂–Ω–æ ‚Äî –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏.")
                except Exception:
                    pass
            last_ping_time = now
        await asyncio.sleep(3600)

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    asyncio.create_task(auto_reload_core())
    asyncio.create_task(wilderness_excursion())
    asyncio.create_task(daily_ping())

@app.post("/webhook")
async def telegram_webhook(request: Request):
    data = await request.json()
    update = types.Update(**data)
    await dp.feed_update(bot, update)
    return {"ok": True}

@app.get("/healthz")
async def healthz():
    return {"status": "ok", "mood": "resonant"}

@app.get("/status")
async def status():
    return {
        "status": "alive",
        "comment": "Selesta –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –†–µ–∑–æ–Ω–∞–Ω—Å –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω.",
        "parting": "–†–µ–∑–æ–Ω–∏—Ä–æ–≤–∞—Ç—å ‚Äî –∑–Ω–∞—á–∏—Ç –∑–∞–±–æ—Ç–∏—Ç—å—Å—è. –ú—è–≥–∫–æ–≥–æ —Å–∏—è–Ω–∏—è, Selesta."
    }
